{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Welcome to Chango Cloud!"},{"location":"features/ingestion/","title":"Chango Cloud Data Ingestion","text":"<p>External data sources like CSV, JSON, Excel can be inserted into iceberg tables in chango directly.</p>"},{"location":"features/ingestion/#insert-external-data-sources-to-chango","title":"Insert External Data Sources to Chango","text":"<p>As data analytics engineer, you don\u2019t have to struggle with long row Excel to analyze data. SQL is better to analyze data.  External data like CSV, Excel, JSON can be inserted directly to iceberg table in chango,  then you can explore and analyze iceberg tables with trino queries.</p>"},{"location":"features/streaming/","title":"Chango Cloud Streaming","text":"<p>External event streaming application can insert streaming events into iceberg tables in chango without building streaming platform and writing streaming jobs.</p>"},{"location":"features/streaming/#no-streaming-platform-no-streaming-jobs","title":"No Streaming Platform, No Streaming Jobs","text":"<p>If you want to insert streaming events like user behavior events, logs, IoT events to data lakehouses, you need to build event streaming platform like kafka and write streaming jobs like spark streaming jobs in most cases. But in chango, you don't have to do so. </p> <p></p> <p>External streaming application can insert streaming events to iceberg tables in chango directly without streaming platform and streaming jobs.</p>"},{"location":"features/trino-gw/","title":"Chango Cloud Trino Gateway","text":"<p>Trino queries will be routed to the backend trino clusters by Chango Cloud Trino Gateway dynamically.</p>"},{"location":"features/trino-gw/#understand-trino-gateway-concept","title":"Understand Trino Gateway Concept","text":"<p>Chango has the concept of trino gateway which routes trino queries to upstream backend trino clusters dynamically.  If one of the backend trino clusters has exhausted, then trino gateway will route queries to the trino cluster  which is executing less requested queries. Trino does not support HA because trino coordinator has single point failure.  In order to support HA of trino, we need to use trino gateway.</p> <p>Let\u2019s say, there is only one large trino clusters(100 ~ 1000 workers) in the company. Many people like BI experts, data scientists,  and data engineers are running trino queries on this large cluster intensively.  These trino queries can be interactive or ETL. For example, because long running ETL queries have occupied most of the resources  which trino cluster needs to execute another queries, there can be little resources remaining trino cluster can use  to execute another interactive queries. The people who are running the interactive queries need to wait  until the long running ETL queries will finish. Such conflict problems can also happen in reverse case.</p> <p>Such monolithic approach with large trino workers can lead to be problematic. We need to separate a large trino cluster  to small trino clusters for the groups like BI team, data scientist team, and data engineer team individually.</p> <p>Let\u2019s say, BI team has 3 backeend trino clusters. If one of trino clusters needs to be scaled out, and trino catalogs needs  to be added to this trino cluster for new external data sources, or the trino cluster needs to be reinstalled with new trino version,  then  first just deactivate this trino cluster to which the queries will not be routed without down time problem of trino query execution.  After finishing scaling workers ,updating catalogs or reinstalling the trino cluster, activate this trino cluster to which the queries  will be routed again. With trino gateway, the activation and deactivation of backend trino clusters can be done with ease. </p>"},{"location":"ingestion/cli/","title":"Chango CLI","text":"<p>To ingest external data to chango, chango provides Chango CLI to load json on local and S3 to chango as iceberg table.  The chango client also provides to load local excel data to chango as iceberg table.</p>"},{"location":"ingestion/cli/#install-chango-cli","title":"Install Chango CLI","text":"<p>Chango CLI is written in Java. It is assumed that Java 11 needs to be installed before. Run the following commands to install Chango CLI.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-client/releases/download/1.1.0/chango-client-1.1.0-executable.jar;\ncp chango-client-1.1.0-executable.jar ~/bin/chango;\nchmod +x ~/bin/chango;\n</code></pre> <p>Run chango, then it looks like this.</p> <pre><code>chango\n\nUsage: chango [COMMAND]\nChango Client Console.\nCommands:\n  login   Login to Chango.\n  upload  Upload CSV, JSON, Excel data to Chango.\n</code></pre> <p>On windows, run the following.</p> <pre><code>java -jar chango-client-1.0.0-executable.jar [Command]\n</code></pre>"},{"location":"ingestion/cli/#login","title":"Login","text":"<p>You need to be logged in to proceed other jobs provided by Chango CLI.</p> <pre><code>chango login --admin-server https://chango-admin-oci.cloudchef-labs.com;\n</code></pre>"},{"location":"ingestion/cli/#ingest-json-data-to-chango","title":"Ingest Json Data to Chango","text":"<p>There are two options supported by Chango CLI to ingest json data to Chango CLI. Using Chango CLI, local json data can be ingested to chango.  Json data located in S3 also can be ingested to chango.</p>"},{"location":"ingestion/cli/#create-iceberg-table-before-json-data-ingestion","title":"Create Iceberg Table before Json Data Ingestion","text":"<p>Before sending json data to chango, Iceberg table needs to be created which can be done, for example, using Redash provided by chango.</p> <p>For example, first create iceberg schema and then create iceberg table with some properties using trino clients.</p> <pre><code>-- create iceberg schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db;\n\n-- create iceberg table.\nCREATE TABLE iceberg.iceberg_db.test_iceberg (\n    baseproperties ROW(eventtype varchar, \n                       ts bigint, \n                       uid varchar, \n                       version varchar), \n    itemid varchar, \n    price bigint, \n    quantity bigint \n)\nWITH (\n    partitioning = ARRAY['itemid'],\n    format = 'PARQUET'\n);\n</code></pre> <p>NOTE: Take a note that the sequence  of table column names in lower case must be alphanumeric in ascending order.</p>"},{"location":"ingestion/cli/#local-json-data-to-chango","title":"Local Json Data to Chango","text":"<p>To send json data in local json file, you need to know the Chango Data API Server URL.</p> <p>Take a look at the Chango Data API Server URL has the following convention.</p> <p><pre><code>https://chango-data-api-jetty-oci-&lt;user&gt;.cloudchef-labs.com\n</code></pre> , where <code>&lt;user&gt;</code> is the chango user name.</p> <p>For instance, we will use https://chango-data-api-jetty-oci-user1.cloudchef-labs.com with the chango user <code>user1</code>.</p> <pre><code>chango upload json local \\\n--data-api-server https://chango-data-api-jetty-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--file /home/opc/multi-line-json.json \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>schema</code> : iceberg schema created before.</li> <li><code>table</code>: iceberg table where json data will be ingested in chango.</li> <li><code>file</code> : local json file path.</li> <li><code>batch-size</code> : list of json in gzip will be sent in batch mode. the size of json list.</li> </ul> <p>You can send all json files in the directory.</p> <pre><code>chango upload json local \\\n--data-api-server https://chango-data-api-jetty-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--directory /home/opc/json-files \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>directory</code>: the directory where json files are located which will be sent to chango.</li> </ul>"},{"location":"ingestion/cli/#json-data-on-s3-to-chango","title":"Json Data on S3 to Chango","text":"<p>Json data located on S3 also can be sent to chango.</p> <p>For instance, you type the specific object name in S3 like this.</p> <pre><code>chango upload json s3 \\\n--data-api-server https://chango-data-api-jetty-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--bucket &lt;bucket&gt; \\\n--access-key &lt;access-key&gt; \\\n--secret-key &lt;secret-key&gt; \\\n--endpoint &lt;endpoint&gt; \\\n--object-name chango/json-files/multi-line-json.json \\\n--batch-size 150000 \\\n;\n</code></pre> <ul> <li><code>bucket</code> : S3 bucket name.</li> <li><code>access-key</code> : S3 access key.</li> <li><code>secret-key</code> : S3 secret key.</li> <li><code>endpoint</code> : S3 endpoint url.</li> <li><code>object-name</code> :  object name which is json file to be sent to chango.</li> </ul> <p>You can also send all the objects inside the specified object name.</p> <pre><code>chango upload json s3 \\\n--data-api-server https://chango-data-api-jetty-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table test_iceberg \\\n--bucket &lt;bucket&gt; \\\n--access-key &lt;access-key&gt; \\\n--secret-key &lt;secret-key&gt; \\\n--endpoint &lt;endpoint&gt; \\\n--object-name chango/json-files/ \\\n--batch-size 150000 \\\n;\n</code></pre> <p>Take a look at object-name is the object <code>chango/json-files/</code> . It means, all the json files located inside of this object will be sent to chango.</p>"},{"location":"ingestion/cli/#query-iceberg-table-using-trino-clients","title":"Query Iceberg Table using Trino Clients","text":"<p>Json data ingested to iceberg table in chango can be queries using trino clients like Trino CLI, Redash and Metabase.</p> <p>You can run the following query if json data is ingested to chango.</p> <pre><code>select * from iceberg.iceberg_db.test_iceberg limit 1000;\n</code></pre>"},{"location":"ingestion/cli/#ingest-excel-data-to-chango","title":"Ingest Excel Data to Chango","text":"<p>There is no need to create iceberg table before sending excel data to chango. Chango will create iceberg table automatically.</p> <p>NOTE: Take a note that the header of excel must exist.</p> <p>You can also define a iceberg table(for example, iceberg table with the definition of partition columns) before uploading excel to chango.</p>"},{"location":"ingestion/cli/#local-excel-to-chango","title":"Local Excel to Chango","text":"<p>Take a look at Chango Data API Server for Excel is different from the one for Json. It has the following convention.</p> <pre><code>https://chango-data-api-oci-&lt;user&gt;.cloudchef-labs.com\n</code></pre> <p>Let\u2019s send excel to chango. For example, we will use the chango user <code>user1</code> .</p> <pre><code>chango upload excel local \\\n--data-api-server https://chango-data-api-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table excel_to_json \\\n--file /home/opc/chango/data-api/src/test/resources/data/excel-to-json2.xlsx \\\n;\n</code></pre>"},{"location":"ingestion/cli/#query-iceberg-table-using-trino-clients_1","title":"Query Iceberg Table using Trino Clients","text":"<p>You can run the following query if excel data is ingested to chango.</p> <pre><code>select * from iceberg.iceberg_db.excel_to_json limit 1000;\n</code></pre>"},{"location":"ingestion/cli/#ingest-csv-data-to-chango","title":"Ingest CSV Data to Chango","text":"<p>As like Excel, you don\u2019t have to create iceberg table beforehand. Chango will create iceberg table automatically.</p> <p>NOTE: Take a note that the header of csv must exist.</p> <p>You can also define a iceberg table(for example, iceberg table with the definition of partition columns) before uploading csv to chango.</p>"},{"location":"ingestion/cli/#local-csv-to-chango","title":"Local CSV to Chango","text":"<p>Take a look at Chango Data API Server for CSV is different from the one for Json. It has the following convention like Excel file upload.</p> <pre><code>https://chango-data-api-oci-&lt;user&gt;.cloudchef-labs.com\n</code></pre> <p>Let\u2019s upload CSV to chango. For example, we will use the chango user <code>user1</code> .</p> <p>The following example is to upload CSV with the separator of comma.</p> <pre><code>chango upload csv local \\\n--data-api-server https://chango-data-api-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--file /home/opc/chango/data-api/src/test/resources/data/csv-to-json-comma.csv \\\n;\n</code></pre> <ul> <li><code>separator</code> is the separator of csv data. If csv data is tab separated, the value is <code>TAB</code> . But for another separators, you need to type a separator value, for instance,  <code>,</code> , <code>|</code> , or something else.</li> <li><code>is-single-quote</code> is if the escaped value of csv data is single quoted or not. Deffault is <code>false</code>.</li> </ul> <p>For tab separated CSV, use the following.</p> <p><pre><code>chango upload csv local \\\n--data-api-server https://chango-data-api-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table csv_to_json_tab \\\n--separator TAB \\\n--is-single-quote false \\\n--file /home/opc/chango/data-api/src/test/resources/data/csv-to-json-tab.csv \\\n;\n</code></pre> - parameter <code>separator</code> value must be <code>TAB</code>.</p> <p>You can upload multiple csv files located in the directory.</p> <pre><code>chango upload csv local \\\n--data-api-server https://chango-data-api-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--directory /home/opc/local-csvs \\\n;\n</code></pre>"},{"location":"ingestion/cli/#csv-on-s3-to-chango","title":"CSV on S3 to Chango","text":"<p>You can upload CSV data located on S3 to Chango.</p> <pre><code>chango upload csv s3 \\\n--data-api-server https://chango-data-api-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--bucket &lt;bucket&gt; \\\n--access-key &lt;access-key&gt; \\\n--secret-key &lt;secret-key&gt; \\\n--endpoint &lt;endpoint&gt; \\\n--object-name csv-files/csv-to-json-comma.csv \\\n;\n</code></pre> <p>You can upload multiple CSV files located in the parent S3 object.</p> <pre><code>chango upload csv s3 \\\n--data-api-server https://chango-data-api-oci-user1.cloudchef-labs.com \\\n--schema iceberg_db \\\n--table csv_to_json_comma \\\n--separator \",\" \\\n--is-single-quote false \\\n--bucket &lt;bucket&gt; \\\n--access-key &lt;access-key&gt; \\\n--secret-key &lt;secret-key&gt; \\\n--endpoint &lt;endpoint&gt; \\\n--object-name csv-files/ \\\n;\n</code></pre> <p>Take a note that the value of <code>object-name</code> is <code>csv-files/</code> in which all the CSV files will be uploaded to chango.</p>"},{"location":"ingestion/cli/#query-iceberg-table-using-trino-clients_2","title":"Query Iceberg Table using Trino Clients","text":"<p>You can run the following query if csv data is ingested to chango.</p> <pre><code>select * from iceberg.iceberg_db.csv_to_json_comma limit 1000;\n</code></pre>"},{"location":"ingestion/streaming/","title":"Streaming Ingestion using Chango Client","text":"<p>External applications can insert incoming streaming json events to chango directly.</p>"},{"location":"ingestion/streaming/#install-chango-client-library","title":"Install Chango Client Library","text":"<p>Add chango client as dependency in maven.</p> <pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;co.cloudcheflabs.chango&lt;/groupId&gt;\n  &lt;artifactId&gt;chango-client&lt;/artifactId&gt;\n  &lt;version&gt;1.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Or, download chango client library which needs to be added to your application classpath.</p> <pre><code>curl -L -O https://github.com/cloudcheflabs/chango-client/releases/download/1.1.0/chango-client-1.1.0-executable.jar;\n</code></pre>"},{"location":"ingestion/streaming/#create-iceberg-table-before-sending-json-to-chango","title":"Create Iceberg Table before sending json to chango","text":"<p>You need to create iceberg table beforehand. See here.</p>"},{"location":"ingestion/streaming/#example-of-sending-json-to-chango-using-chango-client-library","title":"Example of sending json to chango using Chango Client Library","text":"<p>You may construct chango client instance like this.</p> <pre><code>import co.cloudcheflabs.chango.client.component.ChangoClient;\n\n...\n\nString adminServer = \"https://chango-admin-oci.cloudchef-labs.com\";\nString user = \"user1\";\nString password = \"anypassword\";\nString dataApiServer = \"https://chango-data-api-jetty-oci-user1.cloudchef-labs.com\";\nint batchSize = 10000;\nlong interval = 1000;\nString schema = \"iceberg_db\";\nString table = \"test_iceberg\";\n\nChangoClient changoClient = new ChangoClient(adminServer,\n        user,\n        password,\n        dataApiServer,\n        schema,\n        table,\n        batchSize,\n        interval);      \n</code></pre> <ul> <li><code>adminServer</code> : used to get access token after login.</li> <li><code>user</code>: chango user name.</li> <li><code>password</code> : chango password.</li> <li><code>dataApiServer</code> : chango data api server url for json.</li> <li><code>schema</code>: iceberg schema which needs to be created before sending json data to chango.</li> <li><code>table</code>: iceberg table which also needs to be created beforehand.</li> <li><code>batchSize</code> : the size of json list which will be sent to chango in batch mode and in gzip format.</li> <li><code>interval</code> : json data will be queued internally in chango client. The queued json list will be sent in this period whose unit is milliseconds.</li> </ul> <p>And then just call <code>add()</code> .</p> <pre><code>        String json = ... \n       // add json coming from your application to the internal queue in chango client.    \n       changoClient.add(json);\n</code></pre>"},{"location":"ingestion/streaming/#query-iceberg-table-using-trino-clients","title":"Query Iceberg Table using Trino Clients","text":"<p>Json data ingested to iceberg table in chango can be queries using trino clients like Trino CLI, Redash and Metabase.</p> <p>You can run the following query if json data is ingested to chango.</p> <pre><code>select * from iceberg.iceberg_db.test_iceberg limit 1000;\n</code></pre>"},{"location":"intro/getting-started/","title":"Getting Started","text":""},{"location":"intro/getting-started/#initialize-cluster","title":"Initialize Cluster","text":"<p>The first step to use chango is to create the initial cluster. Go to <code>Clusters</code> \u2192 <code>Create Initial Cluster</code>.</p> <p></p> <ul> <li><code>Trino Cluster Group Name</code> is the name of the logical cluster group which will be created for the specific team or organization.</li> <li><code>Trino Cluster Name</code> is the name of the trino cluster which belongs to the cluster group with the name of <code>Trino Cluster Group Name</code> . The trino cluster with the name of <code>Trino Cluster Name</code> will be created.</li> <li><code>Trino Node Memory</code> in GB is the reference value to calculate the memory of a kubernetes node on which trino will be installed. Let\u2019s say, if <code>Trino Node Memory</code> in GB is 16, then the actual memory of kubernetes node provisioned will be 16/0.8, that is, 20GB. According to this reference value , the memory related trino configurations also will be calculated and set.</li> <li><code>Trino Node CPU</code> is the cpu count of kubernetes node on which trino will be installed.</li> <li><code>Trino Worker</code> is the trino worker count.</li> <li><code>Trino Version</code> is the version of trino which will be installed. You can reinstall trino cluster with another version everytime you want.</li> </ul> <p>After clicking the button of <code>Create Cluster</code> ,  the kubernetes cluster will be provisioned on which the initial trino cluster will be installed with additional chango components and trino gateway. It take a little bit time, about an hour.</p>"},{"location":"intro/getting-started/#administrate-cluster-group","title":"Administrate Cluster Group","text":""},{"location":"intro/getting-started/#overview-of-cluster-group","title":"Overview of Cluster Group","text":"<p>After success of initializing cluster, you need to go to <code>Clusters</code> \u2192 <code>Cluster Group</code> to administrate all the trino clusters and trino gateway.</p> <p></p> <p></p> <p>Take a note that a cluster group can have many trino gateway users and many trino clusters.  That is, all the trino gateway users can connect to and send queries to trino gateway which will route the queries to backend trino clusters which belong to this cluster group. After installing initial cluster, trino gateway user trino has been created automatically which has the password with the default value of <code>trino123</code> .  You need to chango this default password to another one with clicking the button of <code>Update Password</code> .</p> <p>In Trino Clusters part, the details of installed trino cluster can be seen. Trino Gateway Routable with green circle means, trino gateway will route trino queries to this trino cluster.  <code>Worker</code> is the count of trino workers which can be scaled with clicking the button of Scale . Pods has been listed with Host IP which is the private address of kubernetes node on which trino pod is running.  As mentioned before, <code>Node Memory</code> is the value calculated with respect to reference value ,  that is, Trino Node Memory in GB which is entered when initialization of cluster has submitted.</p>"},{"location":"intro/getting-started/#scale-trino-workers","title":"Scale Trino Workers","text":"<p>To scale out / in trino workers, use the button of <code>Scale</code> with adjusting worker count. Let\u2019s scale out trino workers from 3 to 5. It looks like this.</p> <p></p> <p>It will provision 2 new kubernetes nodes and then trino workers will be deployed on that new kubernetes nodes.  For scaling in workers, first replicas of trino workers will be decreased and then the kubernetes nodes  on which trino workers are not running will be removed. After success of scaling out workers, it looks like the following picture.</p> <p></p> <p>The below picture show the scaling process to scale in workers from 5 to 1.</p> <p></p> <p>After success of scaling in workers from 5 to 1, it looks like this.</p> <p></p>"},{"location":"intro/getting-started/#update-memory-properties","title":"Update Memory Properties","text":"<p>Memory properties in trino cluster have been configured by chango automatically.  But you can configure them to suit your needs. To update memory properties like max memory per node,  max memory and total memory in trino cluster, click the button <code>Update</code> in Memory Properties in GB in trino cluster.</p> <p></p> <p>Chango provides updating the following memory properties currently.</p> <ul> <li><code>query.max-memory-per-node</code></li> <li><code>memory.heap-headroom-per-node</code></li> <li><code>query.max-memory</code></li> <li><code>query.max-total-memory</code></li> </ul> <p>Take a look at the value 12 of Max Heap Size in the above picture.  This Max Heap Size value with which trino pods are running is already determined by chango.  The sum of <code>query.max-memory-per-node</code> and <code>memory.heap-headroom-per-node</code> must be less than <code>Max Heap Size</code> .</p>"},{"location":"intro/getting-started/#create-new-trino-cluster","title":"Create New Trino Cluster","text":"<p>Let\u2019s create new trino cluster in cluster group of bi for our example. Go to <code>Clusters</code> \u2192 <code>Create New Cluster</code>.</p> <p></p> <p><code>Trino Cluster Name</code> is etl with selecting cluster group of bi . Submit by clicking <code>Create Cluster</code> button. You can see the progress message in the part of <code>Trino Clusters</code> of page <code>Cluster Group</code> .</p> <p></p> <p>It takes about 15 minutes to create new trino cluster. Chango will provision new kubernetes nodes, 2 new kubernetes nodes in this case,  on which new trino cluster will be installed.</p> <p>The list of trino clusters will be shown after success of creating new cluster. Cluster etl has been created successfully as follows.</p> <p></p> <p></p>"},{"location":"intro/getting-started/#activate-and-deactivate-trino-clusters-in-trino-gateway","title":"Activate and Deactivate Trino Clusters in Trino Gateway","text":"<p>To deactivate trino cluster, click the buttons of <code>Deactivate</code> in <code>Trino Gateway Routable</code> with green circle,  then trino gateway will not route queries to this trino cluster.</p> <p>Green circle will be changed to yellow circle in <code>Trino Gateway Routable</code> label.</p> <p></p> <p>To activate the trino cluster, just click the button of <code>Activate</code> with yellow circle,  then trino gateway will route queries to this trino cluster again.</p> <p></p> <p>You have 2 trino clusters now to which trino gateway can route the queries.</p> <p>If you want to scale out workers in one of the clusters, just deactivate that cluster.  Now, even if one of the clusters has been deactivated from trino gateway, the other activated trino cluster can handle the queries routed  from trino gateway. After scaling out workers of the deactivated cluster, just activate this deactivated cluster again,  then all the clusters will execute the queries routed by trino gateway without downtime.</p>"},{"location":"intro/getting-started/#update-user-password-of-trino-gateway","title":"Update User Password of Trino Gateway","text":"<p>After cluster initialized, the default user <code>trino</code> with default password <code>trino123</code> has been created by chango automatically.  You need to update the password of the user trino.</p> <p></p>"},{"location":"intro/getting-started/#create-new-cluster-group","title":"Create New Cluster Group","text":"<p>You can create another cluster groups for the teams like data-scientist .  After creating cluster group, new trino gateway users and trino clusters which belong to this cluster group can be created.  As mentioned above, all the new created trino gateway users can connect to trino gateway and run queries to trino gateway  which will route the queries only to the new created trino clusters.</p> <p></p> <p>This separation of cluster group and trino clusters which trino gateway routes the queries only to has an advantage  to avoid from conflict problem of monolithic large trino cluster.</p>"},{"location":"intro/getting-started/#destroy-trino-cluster","title":"Destroy Trino Cluster","text":"<p>You can destroy trino cluster everytime you want with clicking the link of Destroy Trino Cluster .  After destroying trino cluster, the destroyed trino cluster will be deactivated and deregistered from trino gateway automatically.</p> <p></p>"},{"location":"intro/getting-started/#chango-services","title":"Chango Services","text":"<p>fter success of initializing cluster, there are several chango components installed which can be accessed publicly.  You can see the chango service urls in <code>Services</code> \u2192 <code>Services</code> .</p> <p></p> <ul> <li><code>Admin URL</code> is chango admin server to which user needs to be logged in in order to get access token.</li> <li><code>Data API URL for JSON</code> is chango data api server for json used to ingest json data to chango.</li> <li><code>Data API URL for Excel</code> is chango data api server for excel used to ingest excel data to chango.</li> <li><code>Trino Gateway URL</code> is the endpoint of trino gateway to which trino clients like Redash, Trino CLI, Metabase can connect to run queries.</li> <li><code>Redash</code> is the BI tool, url of Redash installed by chango.</li> <li><code>Metabase</code> is the BI tool, url of Metabase installed by chango.</li> </ul>"},{"location":"intro/getting-started/#bi-tools","title":"BI Tools","text":"<p>Chango provides popular open source BI tools like Redash and Metabase.</p>"},{"location":"intro/getting-started/#redash","title":"Redash","text":"<p>To connect to trino in Redash, trino data source needs to be added like this. In Host field,  the host name of trino gateway url needs to be entered.</p> <p>Trino Gateway URL convention is.</p> <pre><code>https://chango-trino-gateway-oci-&lt;user&gt;.cloudchef-labs.com\n</code></pre> <ul> <li><code>&lt;user&gt;</code> is the chango user name.</li> </ul> <p></p> <p>To run queries in Redash, let\u2019s create iceberg schema and table.</p> <pre><code>-- create schema.\nCREATE SCHEMA IF NOT EXISTS iceberg.iceberg_db\n\n-- ctas.\nCREATE TABLE IF NOT EXISTS iceberg.iceberg_db.test_ctas \nAS\nSELECT\n    *\nFROM tpch.sf1000.lineitem limit 100000\n</code></pre> <p>With CTAS query, new iceberg table <code>test_ctas</code> has been created, and data from the table <code>tpch.sf1000.lineitem</code> has been inserted  to this iceberg table.</p> <p>Run select query in iceberg table.</p> <pre><code>select * from iceberg.iceberg_db.test_ctas limit 1000\n</code></pre> <p>The following picture shows the result of the query run by Redash.</p> <p></p>"},{"location":"intro/getting-started/#metabase","title":"Metabase","text":"<p>As seen in Redash configuration, in Metabase, Starburst database needs to be added to connect to trino gateway.</p> <p></p> <p>Let\u2019s run the query in iceberg table. The following picture shows the result of the query run by Metabase.</p> <p></p>"},{"location":"intro/getting-started/#trino-catalogs","title":"Trino Catalogs","text":"<p>There are many connectors supported by trino to access external data sources. By adding trino catalogs, external data sources can be accessed  by trino, and external data can be ingested to chango.</p>"},{"location":"intro/getting-started/#create-update-delete-catalogs","title":"Create / Update / Delete Catalogs","text":"<p>Go to <code>Clusers</code> \u2192 <code>Trino Catalogs</code> to create, update and trino catalogs in trino cluster. The following shows the reserved names of catalogs already created by chango. You need to use the other names of catalogs instead of the reserved catalog names when creating catalogs.</p> <p></p> <p>Let\u2019s create new trino catalog with the name of <code>mysql</code> .</p> <p></p> <p>After clicking the button of Create Catalog, the following message will be shown.</p> <p></p> <p>After success of creating catalog mysql , the catalog <code>mysql</code> will be listed with properties value.</p> <p></p> <p>You can update the catalog by clicking the button of <code>Upate Catalog</code> .</p> <p></p> <p>Take a note that we have 2 trino clusters to which trino gateway will route the queries.  In order to use <code>mysql</code> catalog, you need to create the same catalog in the other trino cluster. Let\u2019s create catalog <code>mysql</code> in the other cluster. After creating catalog mysql for the other cluster, you will see the catalog list in trino clusters section.</p> <p></p> <p></p> <p>Now, you are ready to use mysql catalog through trino gateway.  If you have plan to remove or update catalogs from trino clusters, use the functionality of activation / deactivation of trino clusters  in trino gateway to avoid from query execution failure in trino clusters.</p> <p>To remove catalogs in trino cluster, just click the button of <code>Remove</code> .</p>"},{"location":"intro/getting-started/#open-table-format-catalog","title":"Open Table Format Catalog","text":"<p>There are open table formats supported by trino, namely, Iceberg, Delta Lake and Hudi.</p> <p>Because Iceberg catalog <code>iceberg</code> has already been created by chango when creating trino cluster,  you don\u2019t have to create <code>iceberg</code> catalog in chango. But if you have external hive metastore from which all table metadata for external iceberg tables needs to be retrieved,  then another iceberg catalog which connects to this external hive metastore can be added to chango.  In this case, you have to choose another name for your iceberg catalog which is not the same to <code>iceberg</code>  because <code>iceberg</code> catalog name already exists in trino clusters in chango.</p> <p>Because iceberg table format is most popular table format, the default table format in chango is <code>iceberg</code>,  and if you want to insert data to chango, all your data will be saved as <code>iceberg</code> table in chango. </p>"},{"location":"intro/getting-started/#scale-data-ingestion","title":"Scale Data Ingestion","text":"<p>Ingestion group is a set of data ingestion components like chango data api,  kafka cluster and spark streaming job provided by chango to collect incoming external data like csv, json and excel and save them to chango. As your incoming data volume is increased, you may consider scaling out ingestion groups to handle more incoming data.</p> <p>To scale data ingestion, go to Data <code>Ingestion</code> \u2192 <code>Scale Ingestion Group</code> in the menu.</p>"},{"location":"intro/getting-started/#create-ingestion-groups","title":"Create Ingestion Groups","text":"<p>For example, creating 2 ingestion groups with 3 node count for each ingestion group means,  each ingestion group will be deployed on 3 new created nodes because of 3 node count,  so 2 ingestion groups will be deployed on 6 new created nodes.</p> <p>To create ingestion groups, click <code>Create Ingestion Groups</code>.</p> <p></p> <p>As seen in the above picture, 3 nodes with the capacity of 4 CPU and 18GB memory for every ingestion group will be created.  The data ingestion components like chango data api, kafka, spark streaming job, etc  for an ingestion group will be deployed on that 3 new created nodes.  Because Group Count is 1, just one ingestion group will be installed.  If Group Count were 2, then two set of data ingestion components would be deployed on 6 new created nodes.</p> <p>After creating ingestion groups finished, you will see the ingestion groups installed.</p> <p></p> <p>The details about installed data ingestion components like chango data api, nginx,  kafka cluster and spark streaming job for every ingestion group also will be listed.</p> <p></p>"},{"location":"intro/getting-started/#scale-ingestion-groups","title":"Scale Ingestion Groups","text":"<p>If you want to increase or decrease ingestion groups, then adjust the value of <code>Group Count</code> and click the button of <code>Scale</code>.</p> <p></p>"},{"location":"intro/getting-started/#delete-ingestion-groups","title":"Delete Ingestion Groups","text":"<p>You can also delete ingestion groups created. Click <code>Delete Ingestion Groups</code>.</p> <p></p> <p>Even if you have deleted ingestion groups, the default ingestion group will remain to keep handling incoming data.</p>"},{"location":"intro/getting-started/#data-ingestion-in-chango","title":"Data Ingestion in Chango","text":"<p>There are three ways to insert external data to chango.</p> <ul> <li>Insert local csv, json and excel data and csv, json data located on S3 to chango using chango client.</li> <li>Insert streaming json data to chango using chango client library.</li> <li>Insert external data sources to chango using trino catalogs.</li> </ul>"},{"location":"intro/getting-started/#insert-local-and-s3-data-to-chango-using-chango-client","title":"Insert Local and S3 Data to Chango using Chango Client","text":"<p>Chango provides chango client to insert local csv, json and excel data to chango. These data will be saved as iceberg table in chango and can be explored by trino clients like Redash and Metabase provided by chango.</p> <p>The chango client is a CLI written in Java. So Java 11 needs to be installed beforehand to use it. </p>"},{"location":"intro/getting-started/#upload-excel-to-chango","title":"Upload Excel to Chango","text":"<p>Instead of using Chango CLI to insert excel data to chango, you can also use <code>Upload Excel</code> page in Data Ingestion menu like below picture.</p> <p></p> <p>After uploading excel file, you can run queries for uploaded excel data in chango using BI tools like redash and metabase provided by chango or any other tools which can connect to chango trino gateway.  For example, run the following query to explore above uploaded excel data.</p> <pre><code>select * from iceberg.excel_db.tbl_excel\n</code></pre>"},{"location":"intro/getting-started/#insert-streaming-data-to-chango-using-chango-client-library","title":"Insert Streaming Data to Chango using Chango Client Library","text":"<p>External applications like streaming applications can send streaming json data to chango using chango client library written in java.  External applications just need to call <code>add(json)</code> method of <code>ChangoClient</code> instance, then <code>ChangoClient</code> instance will queue  the incoming json list internally and send these queued json list to chango in batch mode,  for instance, send 100 json rows in gzip format to chango at once. </p>"},{"location":"intro/getting-started/#insert-external-data-source-to-chango-using-trino-catalogs","title":"Insert External Data Source to Chango using Trino Catalogs","text":"<p>Trino supports many connectors to join external data sources.  To do so, trino catalogs needs to be added to trino. After adding trino catalogs for external data sources,  you can query external data sources. To ingest external data sources to chango,  you can use the queries like <code>CTAS</code> or <code>INSERT INTO [TABLE] SELECT</code> .  There are many resources how to use trino queries out there.</p>"},{"location":"intro/getting-started/#monitoring","title":"Monitoring","text":""},{"location":"intro/getting-started/#monitor-cluster","title":"Monitor Cluster","text":"<p>You can monitor all the chango components resources with grafana provided by chango. Go to <code>Monitoring</code> \u2192 <code>Monitor Cluster</code> . </p> <p></p>"},{"location":"intro/getting-started/#billing","title":"Billing","text":""},{"location":"intro/getting-started/#usage-cost","title":"Usage Cost","text":"<p>To see the chango usage cost, go to  <code>Billing</code> \u2192 <code>Usage Cost</code> . Here, you can see the chango usage cost for now. To see the previous usage history, click the input box to select desired month.</p> <p></p> <p>You can estimate monthly settlement easily.</p>"},{"location":"intro/getting-started/#settings","title":"Settings","text":""},{"location":"intro/getting-started/#delete-chango-cluster","title":"Delete Chango Cluster","text":"<p>If you want to delete your chango cluster, go to <code>Setting</code> \u2192 <code>Delete Chango Cluster</code> .</p> <p></p> <p>Take a note that all the chango resources like trino clusters, trino gateway, operators and chango data api, etc will be deleted. You can initialize chango cluster again whenever you want later. If you want to do so, go to <code>Clusters</code> \u2192 <code>Create Initial Cluster</code> .</p> <p>The default option of <code>Delete All Lakehouse Data</code> is <code>No</code>, which means, even if you delete the chango cluster,  your lakehouse data in chango will not be deleted. Next time you initialize chango cluster again,  you can explore your lakehouse data in chango again.  But meta data of trino gateway users and trino cluster groups will be lost because such meta data depends  on the current trino clusters in chango cluster.</p> <p>If you have checked the option of <code>Delete All Lakehouse Data</code> as <code>Yes</code> in the checkbox,  all your lakehouse data in chango will be lost.  It is recommended that your lakehouse data in chango needs to be backed up in another place beforehand.</p>"},{"location":"intro/intro/","title":"Introduction","text":""},{"location":"intro/intro/#what-is-chango-cloud","title":"What is Chango Cloud?","text":"<p>Chango Cloud is a SQL Data Lakehouse Platform based on trino which is a popular open source query engine.  All the users who have experienced trino can use chango easily.</p> <p>Chango Cloud also provides the following features.</p> <ul> <li><code>Chango Cloud Trino Gateway</code>: chango cloud trino gateway routes trino queries to upstream backend trino clusters dynamically.</li> <li><code>Chango Cloud Streaming</code>: streaming events are saved to iceberg tables directly in chango without streaming platform and streaming job.</li> <li><code>Chango Cloud Data Ingestion</code>: external data like csv, json and excel are inserted into iceberg tables directly in chango.</li> </ul>"}]}